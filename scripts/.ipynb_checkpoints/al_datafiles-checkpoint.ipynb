{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea676e83",
   "metadata": {},
   "source": [
    "Prepare Labeled and Unlabeled files for active learning experimentation. Prepare for specific tasks: sur(face)Seg(mentation)Gl(o)s, can(nonical)Seg(mentation)Gl(o)s, etc.\n",
    "\n",
    "Maximum of 2000 train. Go down by 500 words till 500, then 100, then 50 words. Test data should not change - always same 200 words. \n",
    "\n",
    "Result:\n",
    "\n",
    "**train/dev** input & seggls output files\n",
    "\n",
    "**test** input & seggls output file: words not in training data\n",
    "\n",
    "**selection** (pseudo unlabeled) input file: all_labeled - train/dev amount\n",
    "\n",
    "**metrics** printout, to create chart of F1 & word accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edfb65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46eda567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(dfile):\n",
    "    print(dfile)\n",
    "    words = [word.strip() for word in open(dfile+'.input', encoding='utf8')]\n",
    "    labels = [label.strip() for label in open(dfile+'.output', encoding='utf8')]\n",
    "    print('Test join data:', labels[:5])\n",
    "    return list(zip(words, labels))\n",
    "    \n",
    "    \n",
    "def split_test(joined_labeled_all):\n",
    "    '''this creates a testset of unique words'''\n",
    "    \n",
    "    uniq_labeled_all = list(set(joined_labeled_all))\n",
    "    total = len(uniq_labeled_all)\n",
    "    print('Labeled types: ', total)\n",
    "    statistics = 'Labeled types: '+str(total) + '\\n' + 'Test size: ' + str(TESTSIZE) + '\\n'\n",
    "    \n",
    "    return uniq_labeled_all[:TESTSIZE], statistics\n",
    "\n",
    "\n",
    "def check_overlap(testpairs, allpairs):\n",
    "    return [pair for pair in allpairs if pair not in testpairs]\n",
    "\n",
    "def get_unlabeled(uData):\n",
    "    # get words from unused labeled set\n",
    "    return list(zip(*uData))[0]\n",
    "\n",
    "def inout_file_prep(paired_set, designation='all'):\n",
    "    list(list(zip(*paired_set))[0])\n",
    "    \n",
    "def dev_split(trainset):\n",
    "    return trainset[:len(trainset)//10]\n",
    "\n",
    "def unjoin(joined_data):\n",
    "    unjoined = list(zip(*joined_data))\n",
    "    return unjoined[0], unjoined[1]\n",
    "                \n",
    "def write_inout(joined_dataset, filename):\n",
    "    \"Write input and output files from zipped word and labels\"\n",
    "    in_dataset, out_dataset = unjoin(joined_dataset)\n",
    "    with open(STOREDIR+filename+'.input', 'w', encoding='utf8') as I:\n",
    "        I.write('\\n'.join(in_dataset))\n",
    "    with open(STOREDIR+filename+'.output','w', encoding='utf8') as O:\n",
    "        O.write('\\n'.join(out_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7e762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lang,task,datafile):\n",
    "    stats = lang + task + \" \" + CHAR + '\\n'\n",
    "    \n",
    "    # zip in/x and out,y data to make search for types easier\n",
    "    joined = join_data(datafile)\n",
    "    random.shuffle(joined)\n",
    "    print('Total vocab: ', len(joined))\n",
    "    stats += '\\nTotal vocab: ' + str(len(joined))+ '\\n'\n",
    "    \n",
    "    # split test data \n",
    "    testset, newstats = split_test(joined)\n",
    "    stats += newstats \n",
    "    write_inout(testset, 'test.'+lang+task+CHAR)\n",
    "\n",
    "    # make sure train data has no overlap with test data\n",
    "    trainingpool = check_overlap(testset, joined)\n",
    "    num_train = len(trainingpool)\n",
    "    stats += 'Labeled (train/select) tokens: ' + str(num_train)\n",
    "    stats += '\\n'\n",
    "    \n",
    "    # from here on, it's mostly the same thing for each dataset size\n",
    "    # make train and select files \n",
    "    for size in TRAINSIZES:\n",
    "        if size >= num_train:\n",
    "            size = num_train \n",
    "        # train/dev split\n",
    "        #devsize = size//10\n",
    "        #dev_set = trainingpool[:devsize]\n",
    "        #train_set = trainingpool[devsize:size]\n",
    "        train_set = trainingpool[:size]\n",
    "        # selection data is the data not in current training data\n",
    "        selection_data = trainingpool[size:]\n",
    "        select_size = len(selection_data)\n",
    "        if select_size < 25:\n",
    "            print('NO SELECT DATA for', lang)\n",
    "            \n",
    "        # files\n",
    "        code = str(size) + CHAR\n",
    "        write_inout(train_set, 'train.'+lang+task+code)\n",
    "        #write_inout(dev_set, 'dev.'+lang+ext)\n",
    "        # create select files\n",
    "        write_inout(selection_data, 'select.'+lang+task+code)\n",
    "        with open(STOREDIR+lang+task+CHAR+'_STATS.txt', 'w') as S:\n",
    "            S.write(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e66b8e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/bdg/bdg_L_surSeg\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/bdg/bdg_L_surSeg\n",
      "Test join data: ['biri -i', 'ihi', 'egas', 'm- olok', 'ou']\n",
      "Total vocab:  21616\n",
      "Labeled types:  3034\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/bdg/bdg_L_surSegGls\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/bdg/bdg_L_surSegGls\n",
      "Test join data: ['biri#give -i#ISA.MRK.UV.IMP', 'ihi#kami', 'egas#beras', 'm-#ATTR.ST olok#ketakutan', 'ou#saya;.aku']\n",
      "Total vocab:  20223\n",
      "Labeled types:  2411\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/bdg/bdg_L_gls\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/bdg/bdg_L_gls\n",
      "Test join data: ['give ISA.MRK.UV.IMP', 'kami', 'beras', 'ATTR.ST ketakutan', 'saya;.aku']\n",
      "Total vocab:  20223\n",
      "Labeled types:  2379\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/lez/lez_L_surSeg\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/lez/lez_L_surSeg\n",
      "Test join data: ['са', 'юкъ -у -з', 'зун', 'хуьр -я -й', 'кцӏар -и -з']\n",
      "Total vocab:  13953\n",
      "Labeled types:  3472\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/lez/lez_L_surSegGls\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/lez/lez_L_surSegGls\n",
      "Test join data: ['са#one', 'хуьр#village -я#IN -й#EL', 'хкв#return -езва#IMPF -й#PTCP', 'тир#was', 'са#one']\n",
      "Total vocab:  11211\n",
      "Labeled types:  2605\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/lez/lez_L_gls\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/lez/lez_L_gls\n",
      "Test join data: ['one', 'village IN EL', 'return IMPF PTCP', 'was', 'one']\n",
      "Total vocab:  11211\n",
      "Labeled types:  2595\n",
      "NO SELECT DATA for lez\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-95594a6990cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mlfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'./'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mDATA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-7122aae8b4c8>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(lang, task, datafile)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m#write_inout(dev_set, 'dev.'+lang+ext)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# create select files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mwrite_inout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselection_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'select.'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOREDIR\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mCHAR\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_STATS.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-cccad92194eb>\u001b[0m in \u001b[0;36mwrite_inout\u001b[1;34m(joined_dataset, filename)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwrite_inout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoined_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;34m\"Write input and output files from zipped word and labels\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0min_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoined_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOREDIR\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.input'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mI\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-cccad92194eb>\u001b[0m in \u001b[0;36munjoin\u001b[1;34m(joined_data)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoined_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0munjoined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mjoined_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0munjoined\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munjoined\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwrite_inout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoined_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "STOREDIR = r'C:/Users/thesa/Documents/GitHub/al_morphseg/al_trainselect/'\n",
    "LANGFOLDERS = ['btz','cho',] #'lez_2022_NV','nyb',\n",
    "#LANGFOLDERS = ['bdg','lez','ntu','tau']\n",
    "DATA = ['btz_L','cho_L'] #'nyb',\n",
    "#DATA = ['bdg_L','lez_L','ntu_L','tau_L'] #'nyb',\n",
    "TRAINSIZES = [2500,2000,1500,1000,500,100,50]\n",
    "#TRAINSIZES = [10000,8000,6000,4000,2500,2000,1500,1000,500,100,50]\n",
    "TASKS = ['_surSeg', '_surSegGls', '_gls']\n",
    "TESTSIZE = 200\n",
    "#SELECT_SIZES = [25,50,75,100,150,200]\n",
    "CHAR = '' # treatment of combining characters\n",
    "    \n",
    "for lang,language in enumerate(LANGFOLDERS):\n",
    "    for task in TASKS:\n",
    "        if CHAR == '':\n",
    "            lfilepath = r'C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/'+language+'/'+DATA[lang]+task\n",
    "        else:\n",
    "            lfilepath = r'./'+language+'/'+DATA[lang]+task\n",
    "        main(language,task,lfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9d426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
