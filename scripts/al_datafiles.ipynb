{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea676e83",
   "metadata": {},
   "source": [
    "Prepare Labeled and Unlabeled files for active learning experimentation. Prepare for specific tasks: sur(face)Seg(mentation)Gl(o)s, can(nonical)Seg(mentation)Gl(o)s, etc.\n",
    "\n",
    "Test data should not change - always same 200 words. \n",
    "\n",
    "Result:\n",
    "\n",
    "**train/dev** input & output files\n",
    "\n",
    "**test** input & output file: words not in training data\n",
    "\n",
    "**selection** (pseudo unlabeled) input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edfb65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46eda567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(dfile):\n",
    "    print(dfile)\n",
    "    words = [word.strip() for word in open(dfile+'.input', encoding='utf8')]\n",
    "    labels = [label.strip() for label in open(dfile+'.output', encoding='utf8')]\n",
    "    print('Test join data:', labels[:5])\n",
    "    return list(zip(words, labels))\n",
    "    \n",
    "    \n",
    "def split_test(joined_labeled_all):\n",
    "    '''this creates a testset of unique words'''\n",
    "    \n",
    "    uniq_labeled_all = list(set(joined_labeled_all))\n",
    "    total = len(uniq_labeled_all)\n",
    "    print('Labeled types: ', total)\n",
    "    statistics = 'Labeled types: '+str(total) + '\\n' + 'Test size: ' + str(TESTSIZE) + '\\n'\n",
    "    \n",
    "    return uniq_labeled_all[:TESTSIZE], statistics\n",
    "\n",
    "\n",
    "def check_overlap(testpairs, allpairs):\n",
    "    return [pair for pair in allpairs if pair not in testpairs]\n",
    "\n",
    "def get_unlabeled(uData):\n",
    "    # get words from unused labeled set\n",
    "    return list(zip(*uData))[0]\n",
    "\n",
    "def inout_file_prep(paired_set, designation='all'):\n",
    "    list(list(zip(*paired_set))[0])\n",
    "    \n",
    "def dev_split(trainset):\n",
    "    return trainset[:len(trainset)//10]\n",
    "\n",
    "def unjoin(joined_data):\n",
    "    unjoined = list(zip(*joined_data))\n",
    "    return unjoined[0], unjoined[1]\n",
    "                \n",
    "def write_inout(joined_dataset, filename):\n",
    "    \"Write input and output files from zipped word and labels\"\n",
    "    in_dataset, out_dataset = unjoin(joined_dataset)\n",
    "    with open(STOREDIR+filename+'.input', 'w', encoding='utf8') as I:\n",
    "        I.write('\\n'.join(in_dataset))\n",
    "    with open(STOREDIR+filename+'.output','w', encoding='utf8') as O:\n",
    "        O.write('\\n'.join(out_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7e762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lang,task,datafile):\n",
    "    stats = lang + task + \" \" + CHAR + '\\n'\n",
    "    \n",
    "    # zip in/x and out,y data to make search for types easier\n",
    "    joined = join_data(datafile)\n",
    "    random.shuffle(joined)\n",
    "    print('Total vocab: ', len(joined))\n",
    "    stats += '\\nTotal vocab: ' + str(len(joined))+ '\\n'\n",
    "    \n",
    "    # split test data \n",
    "    testset, newstats = split_test(joined)\n",
    "    stats += newstats \n",
    "    write_inout(testset, 'test.'+lang+task+CHAR)\n",
    "\n",
    "    # make sure train data has no overlap with test data\n",
    "    trainingpool = check_overlap(testset, joined)\n",
    "    num_train = len(trainingpool)\n",
    "    stats += 'Labeled (train/select) tokens: ' + str(num_train)\n",
    "    stats += '\\n'\n",
    "    \n",
    "    # from here on, it's mostly the same thing for each dataset size\n",
    "    # make train and select files \n",
    "    for size in TRAINSIZES:\n",
    "        if size >= num_train:\n",
    "            size = num_train \n",
    "        # train/dev split\n",
    "        #devsize = size//10\n",
    "        #dev_set = trainingpool[:devsize]\n",
    "        #train_set = trainingpool[devsize:size]\n",
    "        train_set = trainingpool[:size]\n",
    "        # selection data is the data not in current training data\n",
    "        selection_data = trainingpool[size:]\n",
    "        select_size = len(selection_data)\n",
    "        if select_size < 25:\n",
    "            print('NO SELECT DATA for', lang)\n",
    "            \n",
    "        # files\n",
    "        code = str(size) + CHAR\n",
    "        write_inout(train_set, 'train.'+lang+task+code)\n",
    "        #write_inout(dev_set, 'dev.'+lang+ext)\n",
    "        # create select files\n",
    "        write_inout(selection_data, 'select.'+lang+task+code)\n",
    "        with open(STOREDIR+lang+task+CHAR+'_STATS.txt', 'w') as S:\n",
    "            S.write(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e66b8e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/btz/btz_L_infl\n",
      "Test join data: ['n', 'Prep', 'distrnum', 'n', 'n']\n",
      "Total vocab:  3779\n",
      "Labeled types:  1422\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/cho/cho_L_infl\n",
      "Test join data: ['n TODAY,.THIS.DAY', 'v 1SI', 'adv', 'v 1SI', 'n TODAY,.THIS.DAY']\n",
      "Total vocab:  4221\n",
      "Labeled types:  595\n",
      "C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/lez/lez_L_infl\n",
      "Test join data: ['cardnum', 'n IN EL', 'v IMPF PTCP', 'cop', 'cardnum']\n",
      "Total vocab:  11210\n",
      "Labeled types:  2607\n"
     ]
    }
   ],
   "source": [
    "STOREDIR = r'C:/Users/thesa/Documents/GitHub/al_morphseg/al_trainselect/'\n",
    "\n",
    "LANGFOLDERS = ['btz','cho','lez'] # 'nyb',\n",
    "#LANGFOLDERS = ['bdg','ntu','tau']\n",
    "DATA = ['btz_L','cho_L','lez_L'] #'nyb',\n",
    "#DATA = ['bdg_L','ntu_L','tau_L'] #'nyb',\n",
    "TRAINSIZES = [2500,2000,1500,1000,500,100,50]\n",
    "#TRAINSIZES = [10000,8000,6000,4000,2500,2000,1500,1000,500,100,50]\n",
    "#all tasks '_surSeg', '_surSegGls', '_gls', '_infl'\n",
    "TASKS = ['_infl']\n",
    "TESTSIZE = 200\n",
    "\n",
    "#SELECT_SIZES = [25,50,75,100,150,200]\n",
    "CHAR = '' # treatment of combining characters\n",
    "    \n",
    "for lang,language in enumerate(LANGFOLDERS):\n",
    "    for task in TASKS:\n",
    "        if CHAR == '':\n",
    "            lfilepath = r'C:/Users/thesa/OneDrive - University of Florida/Research/AL/data/'+language+'/'+DATA[lang]+task\n",
    "        else:\n",
    "            lfilepath = r'./'+language+'/'+DATA[lang]+task\n",
    "        main(language,task,lfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9d426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
