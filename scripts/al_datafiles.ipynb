{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea676e83",
   "metadata": {},
   "source": [
    "Prepare Labeled and Unlabeled files for active learning experimentation. Prepare for specific tasks: sur(face)Seg(mentation)Gl(o)s, can(nonical)Seg(mentation)Gl(o)s, etc.\n",
    "\n",
    "Maximum of 2000 train. Go down by 500 words till 500, then 100, then 50 words. Test data should not change - always same 200 words. \n",
    "\n",
    "Result:\n",
    "\n",
    "**train/dev** input & seggls output files\n",
    "\n",
    "**test** input & seggls output file: words not in training data\n",
    "\n",
    "**selection** (pseudo unlabeled) input file: all_labeled - train/dev amount\n",
    "\n",
    "**metrics** printout, to create chart of F1 & word accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfb65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46eda567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(dfile):\n",
    "    print(dfile)\n",
    "    words = [word.strip() for word in open(dfile+'.input', encoding='utf8')]\n",
    "    labels = [label.strip() for label in open(dfile+'.output', encoding='utf8')]\n",
    "    print('Test join data:', labels[:5])\n",
    "    return list(zip(words, labels))\n",
    "    \n",
    "    \n",
    "def split_test(joined_labeled_all):\n",
    "    '''this creates a testset of unique words'''\n",
    "    \n",
    "    uniq_labeled_all = list(set(joined_labeled_all))\n",
    "    total = len(uniq_labeled_all)\n",
    "    print('Labeled types: ', total)\n",
    "    statistics = 'Labeled types: '+str(total) + '\\n' + 'Test size: ' + str(TESTSIZE) + '\\n'\n",
    "    \n",
    "    return uniq_labeled_all[:TESTSIZE], statistics\n",
    "\n",
    "\n",
    "def check_overlap(testpairs, allpairs):\n",
    "    return [pair for pair in allpairs if pair not in testpairs]\n",
    "\n",
    "def get_unlabeled(uData):\n",
    "    # get words from unused labeled set\n",
    "    return list(zip(*uData))[0]\n",
    "\n",
    "def inout_file_prep(paired_set, designation='all'):\n",
    "    list(list(zip(*paired_set))[0])\n",
    "    \n",
    "def dev_split(trainset):\n",
    "    return trainset[:len(trainset)//10]\n",
    "\n",
    "def unjoin(joined_data):\n",
    "    unjoined = list(zip(*joined_data))\n",
    "    return unjoined[0], unjoined[1]\n",
    "                \n",
    "def write_inout(joined_dataset, filename):\n",
    "    \"Write input and output files from zipped word and labels\"\n",
    "    in_dataset, out_dataset = unjoin(joined_dataset)\n",
    "    with open(STOREDIR+filename+'.input', 'w', encoding='utf8') as I:\n",
    "        I.write('\\n'.join(in_dataset))\n",
    "    with open(STOREDIR+filename+'.output','w', encoding='utf8') as O:\n",
    "        O.write('\\n'.join(out_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7e762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lang,task,datafile):\n",
    "    stats = lang + task + \" \" + CHAR + '\\n'\n",
    "    \n",
    "    # zip in/x and out,y data to make search for types easier\n",
    "    joined = join_data(datafile)\n",
    "    random.shuffle(joined)\n",
    "    print('Total vocab: ', len(joined))\n",
    "    stats += '\\nTotal vocab: ' + str(len(joined))+ '\\n'\n",
    "    \n",
    "    # split test data \n",
    "    testset, newstats = split_test(joined)\n",
    "    stats += newstats \n",
    "    write_inout(testset, 'test.'+lang+task+CHAR)\n",
    "\n",
    "    # make sure train data has no overlap with test data\n",
    "    trainingpool = check_overlap(testset, joined)\n",
    "    num_train = len(trainingpool)\n",
    "    stats += 'Labeled (train/select) tokens: ' + str(num_train)\n",
    "    stats += '\\n'\n",
    "    \n",
    "    # from here on, it's mostly the same thing for each dataset size\n",
    "    # make train and select files \n",
    "    for size in TRAINSIZES:\n",
    "        if size >= num_train:\n",
    "            size = num_train \n",
    "        # train/dev split\n",
    "        #devsize = size//10\n",
    "        #dev_set = trainingpool[:devsize]\n",
    "        #train_set = trainingpool[devsize:size]\n",
    "        train_set = trainingpool[:size]\n",
    "        # selection data is the data not in current training data\n",
    "        selection_data = trainingpool[size:]\n",
    "        select_size = len(selection_data)\n",
    "        if select_size < 25:\n",
    "            print('NO SELECT DATA for', lang)\n",
    "            \n",
    "        # files\n",
    "        code = str(size) + CHAR\n",
    "        write_inout(train_set, 'train.'+lang+task+code)\n",
    "        #write_inout(dev_set, 'dev.'+lang+ext)\n",
    "        # create select files\n",
    "        write_inout(selection_data, 'select.'+lang+task+code)\n",
    "        with open(STOREDIR+lang+task+CHAR+'_STATS.txt', 'w') as S:\n",
    "            S.write(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e66b8e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bdg/bdg_L_surSeg\n",
      "./bdg/bdg_L_surSeg\n",
      "Test join data: ['biri -i', 'ihi', 'egas', 'm- olok', 'ou']\n",
      "Total vocab:  21616\n",
      "Labeled types:  3034\n",
      "./bdg/bdg_L_surSegGls\n",
      "./bdg/bdg_L_surSegGls\n",
      "Test join data: ['biri#give -i#ISA.MRK.UV.IMP', 'ihi#kami', 'egas#beras', 'm-#ATTR.ST olok#ketakutan', 'ou#saya;.aku']\n",
      "Total vocab:  20223\n",
      "Labeled types:  2411\n",
      "./bdg/bdg_L_gls\n",
      "./bdg/bdg_L_gls\n",
      "Test join data: ['give ISA.MRK.UV.IMP', 'kami', 'beras', 'ATTR.ST ketakutan', 'saya;.aku']\n",
      "Total vocab:  20223\n",
      "Labeled types:  2408\n",
      "./btz/btz_L_surSeg\n",
      "./btz/btz_L_surSeg\n",
      "Test join data: ['alkisah', 'ni', 'sebuah', 'kute', 'kute']\n",
      "Total vocab:  3839\n",
      "Labeled types:  1425\n",
      "./btz/btz_L_surSegGls\n",
      "./btz/btz_L_surSegGls\n",
      "Test join data: ['alkisah#The.story.is.told', 'ni#in,.at.(space)', 'sebuah#one', 'kute#city', 'kute#city']\n",
      "Total vocab:  3779\n",
      "Labeled types:  1435\n",
      "./btz/btz_L_gls\n",
      "./btz/btz_L_gls\n",
      "Test join data: ['The.story.is.told', 'in,.at.(space)', 'one', 'city', 'city']\n",
      "Total vocab:  3779\n",
      "Labeled types:  1435\n",
      "./cho/cho_L_surSeg\n",
      "./cho/cho_L_surSeg\n",
      "Test join data: ['himak.nittak', 'aha̱tta -li', 'pí', 'aha̱tta -li', 'tokbá']\n",
      "Total vocab:  7793\n",
      "Labeled types:  1990\n",
      "./cho/cho_L_surSegGls\n",
      "./cho/cho_L_surSegGls\n",
      "Test join data: ['himak.nittak#TODAY,.THIS.DAY', 'aha̱tta#stay,.live.(someplace) -li#1SI', 'pí#just', 'aha̱tta#stay,.live.(someplace) -li#1SI', 'himak.nittak#TODAY,.THIS.DAY']\n",
      "Total vocab:  4222\n",
      "Labeled types:  596\n",
      "./cho/cho_L_gls\n",
      "./cho/cho_L_gls\n",
      "Test join data: ['TODAY,.THIS.DAY', 'stay,.live.(someplace) 1SI', 'just', 'stay,.live.(someplace) 1SI', 'TODAY,.THIS.DAY']\n",
      "Total vocab:  4222\n",
      "Labeled types:  596\n",
      "./lez/lez_L_surSeg\n",
      "./lez/lez_L_surSeg\n",
      "Test join data: ['са', 'юкъ -у -з', 'зун', 'хуьр -я -й', 'кцӏар -и -з']\n",
      "Total vocab:  13953\n",
      "Labeled types:  3472\n",
      "./lez/lez_L_surSegGls\n",
      "./lez/lez_L_surSegGls\n",
      "Test join data: ['са#one', 'хуьр#village -я#IN -й#EL', 'хкв#return -езва#IMPF -й#PTCP', 'тир#was', 'са#one']\n",
      "Total vocab:  11211\n",
      "Labeled types:  2605\n",
      "./lez/lez_L_gls\n",
      "./lez/lez_L_gls\n",
      "Test join data: ['one', 'village IN EL', 'return IMPF PTCP', 'was', 'one']\n",
      "Total vocab:  11211\n",
      "Labeled types:  2599\n",
      "./ntu/ntu_L_surSeg\n",
      "./ntu/ntu_L_surSeg\n",
      "Test join data: ['ve ==ä', 'dckta.foks', 'më', 'në- a- mu kä.. tö -kö ==de', 'pawa']\n",
      "Total vocab:  16544\n",
      "Labeled types:  4419\n",
      "./ntu/ntu_L_surSegGls\n",
      "./ntu/ntu_L_surSegGls\n",
      "Test join data: ['ve#accompany ==ä#=1ᴍiɴI', 'më#ᴘʀᴇᴘ', 'në-#ɴᴍʟᴢ1 a-#ᴄᴀᴜЅ mu#eat kä..#ѕᴜʙʀ tö#holy -kö#ɴᴍʟᴢ.ᴘᴄFʟ ==de#=3ᴍiɴII', 'pawa#Pawa.school', 'skul#be.schooled']\n",
      "Total vocab:  15353\n",
      "Labeled types:  3749\n",
      "./ntu/ntu_L_gls\n",
      "./ntu/ntu_L_gls\n",
      "Test join data: ['accompany =1ᴍiɴI', 'ᴘʀᴇᴘ', 'ɴᴍʟᴢ1 ᴄᴀᴜЅ eat ѕᴜʙʀ holy ɴᴍʟᴢ.ᴘᴄFʟ =3ᴍiɴII', 'Pawa.school', 'be.schooled']\n",
      "Total vocab:  15353\n",
      "Labeled types:  3746\n",
      "./tau/tau_L_surSeg\n",
      "./tau/tau_L_surSeg\n",
      "Test join data: ['keey', 'tah', 'hih- nee- shyąą', 'jah', 'dineh']\n",
      "Total vocab:  14099\n",
      "Labeled types:  3913\n",
      "./tau/tau_L_surSegGls\n",
      "./tau/tau_L_surSegGls\n",
      "Test join data: ['keey#village', 'tah#at:AR', 'hih-#3PL.S. nee-#QUAL:DH.PFV:Ø. shyąą#grow:PFV', 'jah#here', 'dineh#person']\n",
      "Total vocab:  11897\n",
      "Labeled types:  2834\n",
      "./tau/tau_L_gls\n",
      "./tau/tau_L_gls\n",
      "Test join data: ['village', 'at:AR', '3PL.S. QUAL:DH.PFV:Ø. grow:PFV', 'here', 'person']\n",
      "Total vocab:  11897\n",
      "Labeled types:  2828\n"
     ]
    }
   ],
   "source": [
    "STOREDIR = r'C:/Users/thesa/Documents/GitHub/al_morphseg/al_trainselect'\n",
    "LANGFOLDERS = ['bdg','btz','cho','lez','ntu','tau'] #'lez_2022_NV','nyb',\n",
    "DATA = ['bdg_L','btz_L','cho_L','lez_L','ntu_L','tau_L'] #'nyb',\n",
    "TASKS = ['_surSeg', '_surSegGls', '_gls']\n",
    "TRAINSIZES = [2000,1500,1000,500,100,50]\n",
    "TESTSIZE = 200\n",
    "SELECT_SIZES = [25,50,75,100,150,200]\n",
    "CHAR = '' # treatment of combining characters\n",
    "    \n",
    "for lang,language in enumerate(LANGFOLDERS):\n",
    "    for task in TASKS:\n",
    "        if CHAR == '':\n",
    "            lfilepath = r'./'+language+'/'+DATA[lang]+task\n",
    "        else:\n",
    "            lfilepath = r'./'+language+'/'+DATA[lang]+task\n",
    "        print(lfilepath)\n",
    "        main(language,task,lfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9d426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
